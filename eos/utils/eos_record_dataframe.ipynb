{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-26T05:48:44.615927827Z",
     "start_time": "2023-07-26T05:48:41.357283151Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "\n",
    "class display(object):\n",
    "    \"\"\"Display HTML representation of multiple objects\"\"\"\n",
    "\n",
    "    template = \"\"\"<div style=\"float: left; padding: 10px;\">\n",
    "    <p style='font-family:\"Courier New\", Courier, monospace'>{0}</p>{1}\n",
    "    </div>\"\"\"\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        self.args = args\n",
    "\n",
    "    def _repr_html_(self):\n",
    "        return \"\\n\".join(\n",
    "            self.template.format(a, eval(a)._repr_html_()) for a in self.args\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"\\n\\n\".join(a + \"\\n\" + repr(eval(a)) for a in self.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-26T05:48:44.668630472Z",
     "start_time": "2023-07-26T05:48:41.357385888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1, 2, 3],\n       [4, 5, 6]])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "array([1, 2, 3, 4, 5, 6])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "array([1, 4, 2, 5, 3, 6])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "a\n",
    "\n",
    "b = a.flatten()\n",
    "b\n",
    "c = a.flatten(\"F\")\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-26T07:00:45.202788898Z",
     "start_time": "2023-07-26T07:00:42.505260418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pool.__init_subclass__(): DocItemT in DBPool from Pool\n",
      "Pool.__init_subclass__(): DataFrame in MongoPool from Pool\n",
      "Pool.__init_subclass__(): RecordDoc in RecordNumpyArrayPool from Pool\n",
      "Pool.__init_subclass__(): DataFrame in DaskPool from Pool\n",
      "Pool.__init_subclass__(): DataFrame in ParquetPool from DaskPool\n",
      "Pool.__init_subclass__(): DataFrame in AvroPool from DaskPool\n",
      "Pool.__init_subclass__(): DocItemT in EpisodeFilePool from Pool\n",
      "Pool.__init_subclass__(): DocItemT in DBBuffer from Buffer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-26 15:00:43.563789: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-26 15:00:43.599705: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-26 15:00:43.600612: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pool.__init_subclass__(): DataFrame in MongoBuffer from Buffer\n",
      "Pool.__init_subclass__(): DataFrame in DaskBuffer from Buffer\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ordered_set import OrderedSet\n",
    "from eos.data_io.config import drivers, trucks_by_id, drivers_by_id\n",
    "\n",
    "# DriverSet = OrderedSet(\n",
    "#     [\n",
    "#         'wang-cheng',\n",
    "#         'li-changlong',\n",
    "#         'chen-hongmei',\n",
    "#         'zheng-longfei',\n",
    "#     ]\n",
    "# )\n",
    "# DriverSet[0]\n",
    "# DriverSet.get_loc('wang-cheng')\n",
    "# DriverSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "ts = pd.to_datetime(datetime.now())\n",
    "ts_ind = ts + pd.to_timedelta(np.arange(2), \"H\")\n",
    "ts_ind\n",
    "df = pd.DataFrame(a, index=ts_ind, columns=[\"c1\", \"c2\", \"c3\"])\n",
    "df\n",
    "df.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts\n",
    "ts_end = pd.to_datetime(datetime.now())\n",
    "ts_end\n",
    "span = ts_end - ts\n",
    "span\n",
    "span_each_row = span / 4\n",
    "span_each_row\n",
    "np.linspace(0, 4, 5)\n",
    "ts_ser = ts + pd.to_timedelta(np.linspace(1, 4, 4) * span_each_row, unit=\"s\")\n",
    "ts_ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts = pd.DataFrame(np.array([ts]), columns=[\"timestamp\"])\n",
    "df_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.arange(10)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = np.arange(12)\n",
    "a1 = ss[:4].tolist()\n",
    "a2 = ss[4:8].tolist()\n",
    "a3 = ss[8:].tolist()\n",
    "ss = [a1, a2, a3]\n",
    "ss\n",
    "df_s = pd.DataFrame(ss, columns=[\"t\", \"v\", \"p\", \"b\"])  # .set_index('t')\n",
    "df_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0, 4 * 20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_ind = ts + pd.to_timedelta(np.arange(0, 4 * 20, 20), \"ms\")\n",
    "ts_ind\n",
    "ss = np.arange(12)\n",
    "a1 = ss[:4]\n",
    "a2 = ss[4:8]\n",
    "a3 = ss[8:]\n",
    "df_ss = pd.DataFrame(\n",
    "    {\"timestep\": ts_ind, \"velocity\": a1, \"thrust\": a2, \"brake\": a3}\n",
    ")  # .set_index('timestep')\n",
    "df_ss.columns.name = \"tuple\"\n",
    "df_ss\n",
    "df_ss[[\"velocity\", \"thrust\", \"brake\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npa_ss_flatten = df_ss[[\"velocity\", \"thrust\", \"brake\"]].to_numpy().flatten()\n",
    "npa_ss_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ui_t = df_ss.loc[:, [\"velocity\", \"thrust\"]]\n",
    "ui_t\n",
    "power_t = ui_t.prod(axis=1)\n",
    "power_t\n",
    "power_t.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = df_ss.stack().swaplevel(0, 1)\n",
    "state.name = \"state\"\n",
    "state.index.names = [\"rows\", \"idx\"]\n",
    "state.sort_index(inplace=True)\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(state)\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# idx = pd.IndexSlice\n",
    "state_stripped = state[[\"velocity\", \"thrust\", \"brake\"]]\n",
    "state_stripped\n",
    "state_stripped.values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functools import reduce\n",
    "#\n",
    "#\n",
    "# velocity = pd.Series(ss[:4], name='velocity')\n",
    "# thrust = pd.Series(ss[4:8], name='thrust')\n",
    "# brake = pd.Series(ss[8:], name='brake')\n",
    "# series = [velocity, thrust, brake]\n",
    "# state = reduce(\n",
    "#     lambda x, y: pd.merge(x, y, how='outer', left_index=True, right_index=True), series\n",
    "# )\n",
    "# state = state.stack().swaplevel(0, 1).sort_index()\n",
    "# state.name = 'state'\n",
    "# state\n",
    "# len(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functools import reduce\n",
    "#\n",
    "#\n",
    "# velocity = pd.Series(s[:4], name='velocity')\n",
    "# thrust = pd.Series(s[4:7], name='thrust')\n",
    "# brake = pd.Series(s[7:], name='brake')\n",
    "# series = [velocity, thrust, brake]\n",
    "# state = reduce(\n",
    "#     lambda x, y: pd.merge(x, y, how='outer', left_index=True, right_index=True), series\n",
    "# )\n",
    "# state = state.stack().swaplevel(0, 1).sort_index()\n",
    "# state.name = 'state'\n",
    "# state\n",
    "# len(state)\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "a = len(ss) + np.arange(15)\n",
    "speed_ser = pd.Series(np.linspace(40, 60, 3), name=\"speed\")\n",
    "# row_ser\n",
    "row_dict = {f\"r{i}\": a[i * 5 : i * 5 + 5] for i in np.arange(3)}\n",
    "row_dict\n",
    "row_array = a.reshape(3, 5).transpose()\n",
    "row_array\n",
    "rows_df = pd.DataFrame(row_array)\n",
    "rows_df.columns = [f\"r{i}\" for i in np.arange(3)]\n",
    "rows_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ts_ind = ts + pd.to_timedelta(np.arange(5 * 20, 8 * 20, 20), \"ms\")\n",
    "ts_ser = pd.Series(ts_ind, name=\"timestep\")\n",
    "throttle_ser = pd.Series(np.linspace(0, 1.0, 5), name=\"throttle\")\n",
    "# throttle_ser\n",
    "dfs = [rows_df, ts_ser, speed_ser, throttle_ser]\n",
    "action = (\n",
    "    reduce(\n",
    "        lambda left, right: pd.merge(\n",
    "            left, right, how=\"outer\", left_index=True, right_index=True\n",
    "        ),\n",
    "        dfs,\n",
    "    )\n",
    "    .stack()\n",
    "    .swaplevel(0, 1)\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "action.name = \"action\"\n",
    "action.index.names = [\"rows\", \"idx\"]\n",
    "action.index\n",
    "action\n",
    "# action = pd.concat([df.DataFrame(row_dict),df.Series(row_ind),df.Series(ts_ind),df.Series(pedal_ind)],axis=1)\n",
    "# action\n",
    "# pedal_ind\n",
    "# action_ind = pd.MultiIndex.from_product([row_ind, ts_ind], names=['row', 'timestamp'])\n",
    "# a\n",
    "# speed_row, row_name = *row_dict\n",
    "# action = (\n",
    "#     pd.DataFrame(\n",
    "#         {\n",
    "#             **row_dict,\n",
    "#             'throttle': pedal_ind,\n",
    "#             'speed': [r[1] for r in row_ind],\n",
    "#         }\n",
    "#     )  # .set_index('pedal')\n",
    "#     # .stack()\n",
    "#     # .swaplevel(0, 1)\n",
    "#     .sort_index()\n",
    "# )\n",
    "# action\n",
    "# action.columns\n",
    "# action.columns = pd.MultiIndex.from_arrays(\n",
    "#     [action.columns, ts_ind], names=['nominalspeed', 'timestep']\n",
    "# )\n",
    "# action = action.stack([0, 1]).swaplevel(0, 1).swaplevel(1, 2).sort_index()\n",
    "# action.name = 'action'\n",
    "# # action.index.names = ['nominalspeed', 'timestamp']\n",
    "# # action.index\n",
    "# action"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# action = (\n",
    "#     reduce(\n",
    "#         lambda left, right: pd.merge(\n",
    "#             left, right, how='outer', left_index=True, right_index=True\n",
    "#         ),\n",
    "#         dfs,\n",
    "#     )\n",
    "#     .stack()\n",
    "#     .swaplevel(0, 1)\n",
    "#     .sort_index()\n",
    "# )\n",
    "#\n",
    "# action.name = 'action'\n",
    "# action.index.names = ['rows', 'idx']\n",
    "# action.index\n",
    "# action\n",
    "action1 = reduce(\n",
    "    lambda left, right: pd.merge(\n",
    "        left, right, how=\"outer\", left_index=True, right_index=True\n",
    "    ),\n",
    "    dfs,\n",
    ")\n",
    "action1\n",
    "action2 = action1.stack()\n",
    "type(action2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ts\n",
    "span = (ts_ind[1] - ts) / 5\n",
    "# span\n",
    "timestep = pd.to_datetime(ts + np.linspace(1, 5, 5) * span)\n",
    "timestep\n",
    "timeseries = pd.Series(timestep, name=\"timestep\")\n",
    "timeseries\n",
    "isinstance(timeseries, pd.Series)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = len(state) + np.arange(6)\n",
    "# ar0 = pd.Series(a[:2], name='row0')\n",
    "# ar1 = pd.Series(a[2:4], name='row1')\n",
    "# ar2 = pd.Series(a[4:], name='row2')\n",
    "# series = [ar0, ar1, ar2]\n",
    "# action = reduce(\n",
    "#     lambda x, y: pd.merge(x, y, how='outer', left_index=True, right_index=True), series\n",
    "# )\n",
    "# action = action.stack().swaplevel(0, 1).sort_index()\n",
    "# action.name = 'action'\n",
    "# action\n",
    "# len(action) + len(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = (\n",
    "    pd.DataFrame({\"work\": len(ss) + len(a), \"timestep\": ts_ind[0]}, index=[0])\n",
    "    .stack()\n",
    "    .swaplevel(0, 1)\n",
    "    .sort_index()\n",
    ")\n",
    "# reward_index = (reward.name,  ts_ind[0], 0)\n",
    "reward.index.names = [\"rows\", \"idx\"]\n",
    "reward.name = \"reward\"\n",
    "reward.index\n",
    "reward\n",
    "# reward.name = 'reward'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward[\"work\"][0]\n",
    "reward.loc[(\"work\", 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# velocity = pd.Series(len(state) + len(action) + len(reward) + s[:4], name='velocity')\n",
    "# thrust = pd.Series(len(state) + len(action) + len(reward) + s[4:7], name='thrust')\n",
    "# brake = pd.Series(len(state) + len(action) + len(reward) + s[7:], name='brake')\n",
    "# series = [velocity, thrust, brake]\n",
    "# nstate = reduce(\n",
    "#     lambda x, y: pd.merge(x, y, how='outer', left_index=True, right_index=True), series\n",
    "# )\n",
    "# nstate = nstate.stack().swaplevel(0, 1).sort_index()\n",
    "# nstate.name = 'nstate'\n",
    "# nstate\n",
    "# len(nstate)\n",
    "ts_ind = ts + pd.to_timedelta(5, \"s\") + pd.to_timedelta(np.arange(0, 4 * 20, 20), \"ms\")\n",
    "ts_ind\n",
    "ss = (\n",
    "    np.arange(12) + len(ss) + len(a) + len(reward) - 1\n",
    ")  # exclude the timestamp in reward\n",
    "a1 = ss[:4]\n",
    "a2 = ss[4:8]\n",
    "a3 = ss[8:]\n",
    "nstate = (\n",
    "    pd.DataFrame({\"timestep\": ts_ind, \"velocity\": a1, \"thrust\": a2, \"brake\": a3})\n",
    "    # .set_index('timestamp')\n",
    "    .stack()\n",
    "    .swaplevel(0, 1)\n",
    "    .sort_index()\n",
    ")\n",
    "nstate.name = \"nstate\"\n",
    "nstate.index.names = [\"rows\", \"idx\"]\n",
    "len(nstate)\n",
    "nstate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = pd.Series([ts], name=\"timestamp\")\n",
    "timestamp.index = pd.MultiIndex.from_product(\n",
    "    [timestamp.index, [0]], names=[\"rows\", \"idx\"]\n",
    ")\n",
    "timestamp.index\n",
    "state.index\n",
    "action.index\n",
    "reward.index\n",
    "nstate.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_index = (timestamp.name, \"\", 0)\n",
    "# timestamp_index\n",
    "state_index = [(state.name, *i) for i in state.index]\n",
    "# state_index\n",
    "reward_index = [(reward.name, *i) for i in reward.index]\n",
    "# reward_index\n",
    "action_index = [(action.name, *i) for i in action.index]\n",
    "# action_index\n",
    "nstate_index = [(nstate.name, *i) for i in nstate.index]\n",
    "# nstate_index\n",
    "\n",
    "multiindex = pd.MultiIndex.from_tuples(\n",
    "    [timestamp_index, *state_index, *action_index, *reward_index, *nstate_index]\n",
    ")\n",
    "multiindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_list = [timestamp, state, action, reward, nstate]\n",
    "observation = pd.concat(observation_list)\n",
    "observation.index = multiindex\n",
    "observation\n",
    "observation.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(observation)\n",
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation0 = observation.copy()\n",
    "observation0.loc[\"timestamp\", \"\", 0] = ts + pd.Timedelta(1, \"h\")\n",
    "observation1 = observation.copy()\n",
    "observation1.loc[\"timestamp\", \"\", 0] = ts + pd.Timedelta(2, \"h\")\n",
    "observation2 = observation.copy()\n",
    "observation2.loc[\"timestamp\", \"\", 0] = ts + pd.Timedelta(3, \"h\")\n",
    "observation3 = observation.copy()\n",
    "observation3.loc[\"timestamp\", \"\", 0] = ts + pd.Timedelta(4, \"h\")\n",
    "observation4 = observation.copy()\n",
    "observation4.loc[\"timestamp\", \"\", 0] = ts + pd.Timedelta(5, \"h\")\n",
    "observation_list = [\n",
    "    observation0,\n",
    "    observation1,\n",
    "    observation2,\n",
    "    observation3,\n",
    "    observation4,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract state action reward nstate from list of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "observation_list[0]\n",
    "observation_list[0].index\n",
    "observation_list[0].loc[idx[\"state\", [\"velocity\", \"thrust\", \"brake\"]]].values\n",
    "observation_list[0].loc[idx[\"action\", [\"r0\", \"r1\", \"r2\"]]].values\n",
    "observation_list[0].loc[idx[\"reward\", [\"work\"]]].values\n",
    "observation_list[0].loc[idx[\"nstate\", [\"velocity\", \"thrust\", \"brake\"]]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "state = []\n",
    "action = []\n",
    "reward = []\n",
    "nstate = []\n",
    "for observation in observation_list:\n",
    "    state.append(observation.loc[idx[\"state\", [\"velocity\", \"thrust\", \"brake\"]]].values)\n",
    "    action.append(observation.loc[idx[\"action\", [\"r0\", \"r1\", \"r2\"]]].values)\n",
    "    reward.append(observation.loc[idx[\"reward\", [\"work\"]]].values)\n",
    "    nstate.append(\n",
    "        observation.loc[idx[\"nstate\", [\"velocity\", \"thrust\", \"brake\"]]].values\n",
    "    )\n",
    "npa_state = np.stack(state)\n",
    "npa_action = np.stack(action)\n",
    "npa_reward = np.stack(reward)\n",
    "npa_nstate = np.stack(nstate)\n",
    "npa_state\n",
    "npa_action\n",
    "npa_reward\n",
    "npa_nstate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "npa_state = np.stack(state)\n",
    "type(npa_state)\n",
    "npa_state.shape\n",
    "state_npa = np.array(state)\n",
    "type(state_npa)\n",
    "state_npa.shape\n",
    "npa_state == state_npa\n",
    "npa_vstate = np.vstack(state)\n",
    "npa_vstate.shape\n",
    "type(npa_vstate)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "npa_nstate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "npa_action"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "npa_reward"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_epi = pd.concat(observation_list, axis=1).transpose()\n",
    "dfs_epi.columns.names = [\"tuple\", \"rows\", \"idx\"]\n",
    "dfs_epi.columns\n",
    "dfs_epi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episode1 = dfs_epi.copy().sort_index(axis=1)\n",
    "dfs_episode1.set_index((\"timestamp\", \"\", 0), inplace=True)\n",
    "dfs_episode1.index.name = \"timestamp\"\n",
    "idx = pd.IndexSlice\n",
    "dfs_episode1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episode1.loc[:, idx[\"action\":\"state\", :, :, :]] = dfs_episode1.loc[\n",
    "    :, idx[\"action\":\"state\", :, :, :]\n",
    "].astype(\"int\")\n",
    "dfs_episode1\n",
    "dfs_episode1.dtypes\n",
    "dfs_episode1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert columns types to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_epi\n",
    "dfs_episode = dfs_epi.copy()\n",
    "dfs_episode.index\n",
    "dfs_episode.set_index((\"timestamp\", \"\", 0), inplace=True)\n",
    "dfs_episode.sort_index(axis=1, inplace=True)\n",
    "dfs_episode.index\n",
    "dfs_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episode.index.name = \"timestamp\"\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "# # dfs_episode = dfs_episode.astype({idx['state', 'velocity', :]: 'int'})  # Error unhashable type: 'slice' for dictionary\n",
    "# # sorted_idx = **(idx['state', :, :]:'int')\n",
    "# # sorted_idx\n",
    "# dfs_episode.loc[:, idx['state', ['brake', 'thrust', 'velocity'], :]] = dfs_episode.loc[\n",
    "#     :, idx['state', ['brake', 'thrust', 'velocity'], :]\n",
    "# ].astype(\n",
    "#     'float'\n",
    "# )  # float16 not allowed in parquet\n",
    "#\n",
    "# dfs_episode.loc[:, idx['action', ['r0', 'r1', 'r2'], :]] = dfs_episode.loc[\n",
    "#     :, idx['action', ['r0', 'r1', 'r2'], :]\n",
    "# ].astype(\n",
    "#     'float'\n",
    "# )  # float16 not allowed in parquet\n",
    "# # dfs_episode['action'] = dfs_episode['action'].astype('float')\n",
    "# # dfs_episode['reward'] = dfs_episode['reward'].astype('float')\n",
    "# dfs_episode.loc[:, idx['reward', 'work', 0]] = dfs_episode.loc[\n",
    "#     :, idx['reward', 'work', 0]\n",
    "# ].astype(\n",
    "#     'float'\n",
    "# )  # float16 not allowed in parquet\n",
    "#\n",
    "# # dfs_episode['nstate'] = dfs_episode['nstate'].astype('float')\n",
    "# dfs_episode.loc[:, idx['nstate', ['brake', 'thrust', 'velocity'], :]] = dfs_episode.loc[\n",
    "#     :, idx['nstate', ['brake', 'thrust', 'velocity'], :]\n",
    "# ].astype(\n",
    "#     'float'\n",
    "# )  # float16 not allowed in parquet\n",
    "state_cols_float = [(\"state\", col) for col in [\"brake\", \"thrust\", \"velocity\"]]\n",
    "action_cols_float = [(\"action\", col) for col in [\"r0\", \"r1\", \"r2\", \"speed\", \"throttle\"]]\n",
    "reward_cols_float = [(\"reward\", \"work\")]\n",
    "nstate_cols_float = [(\"nstate\", col) for col in [\"brake\", \"thrust\", \"velocity\"]]\n",
    "for col in action_cols_float + state_cols_float + reward_cols_float + nstate_cols_float:\n",
    "    dfs_episode[col[0], col[1]] = dfs_episode[col[0], col[1]].astype(\n",
    "        \"float\"\n",
    "    )  # float16 not allowed in parquet\n",
    "dfs_episode\n",
    "dfs_episode.dtypes\n",
    "# dfs_episode.columns\n",
    "# dfs_epi\n",
    "# dfs_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepend two levels of index \"vehicle\" and \"driver\" to the DataFrame object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episode = pd.concat(\n",
    "    [dfs_episode], keys=[drivers_by_id[\"wang-cheng\"].pid], names=[\"driver\"]\n",
    ")\n",
    "dfs_episode = pd.concat(\n",
    "    [dfs_episode], keys=[trucks_by_id[\"VB7\"].vid], names=[\"vehicle\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episode.index\n",
    "dfs_episode.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs_episode.columns.set_names(\n",
    "#     [\n",
    "#         'tuple',\n",
    "#         'rows',\n",
    "#         'idx',\n",
    "#     ],\n",
    "#     level=[0, 1, 2],\n",
    "#     inplace=True,\n",
    "# )\n",
    "dfs_episode\n",
    "dfs_episode.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs_episode['state', 'thrust'] = dfs_episode['state', 'thrust'].astype('float')\n",
    "# # dfs_episode.loc[:, idx['state', ['brake', 'thrust', 'velocity'], :]] = dfs_episode.loc[\n",
    "# #     :, idx['state', ['brake', 'thrust', 'velocity'], :]\n",
    "# # ].astype(\n",
    "# #     'float'\n",
    "# # )  # float16 not allowed in parquet\n",
    "# dfs_episode.loc[:, ('state', ['brake', 'thrust', 'velocity'])] = dfs_episode.loc[\n",
    "#     :, ('state', ['brake', 'thrust', 'velocity'])\n",
    "# ].astype(\n",
    "#     'float'\n",
    "# )  # float16 not allowed in parquet\n",
    "# dfs_episode['action', 'r0'] = dfs_episode['action', 'r0'].astype(\n",
    "#     'float'\n",
    "# )  # float16 not allowed in parquet\n",
    "#\n",
    "# dfs_episode.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = pd.IndexSlice\n",
    "# dfs_episode.loc[:, idx['state', :, :]] = dfs_episode.loc[:, idx['state', :, :]].astype(\n",
    "#     'int'\n",
    "# )\n",
    "# dfs_episode.loc[:, idx['action', :, :]] = dfs_episode.loc[\n",
    "#     :, idx['action', :, :]\n",
    "# ].astype('float16')\n",
    "# dfs_episode.loc[:, idx['reward', :, :]] = dfs_episode.loc[\n",
    "#     :, idx['reward', :, :]\n",
    "# ].astype('float16')\n",
    "# dfs_episode.loc[:, idx['nstate', :, :]] = dfs_episode.loc[\n",
    "#     :, idx['nstate', :, :]\n",
    "# ].astype('float16')\n",
    "#\n",
    "# vel_1 = dfs_episode[[('state', 'velocity', 1)]]\n",
    "# vel_1.dtypes\n",
    "# vel_1.index\n",
    "# vel_1.values\n",
    "# # type(vel_1)\n",
    "# vel_1.iloc[0]\n",
    "# type(vel_1.iloc[0])\n",
    "# type(vel_1.iloc[0].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a level of index for episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodestart = ts - pd.Timedelta(1, \"h\")\n",
    "dfs_episode = pd.concat([dfs_episode], keys=[ts], names=[\"episodestart\"])\n",
    "dfs_episode = dfs_episode.swaplevel(1, 0, axis=0)\n",
    "dfs_episode = dfs_episode.swaplevel(1, 2, axis=0)\n",
    "dfs_episode.sort_index(inplace=True)\n",
    "dfs_episode.index\n",
    "dfs_episode.columns\n",
    "dfs_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episode.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vb7 = trucks_by_id[\"VB7\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eos.data_io.struct import PoolQuery\n",
    "\n",
    "query = PoolQuery(\n",
    "    site=trucks_by_id[\"VB7\"].site,\n",
    "    vehicle=trucks_by_id[\"VB7\"].vid,\n",
    "    driver=drivers_by_id[\"zheng-longfei\"].pid,\n",
    "    start=episodestart,\n",
    "    end=ts,\n",
    ")\n",
    "isinstance(query, dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_new = pd.to_datetime((datetime.now()))\n",
    "ts_new\n",
    "episodestart = ts_new - pd.Timedelta(2, \"d\")\n",
    "episodestart\n",
    "\n",
    "dfs_episode0 = dfs_episode.copy()\n",
    "dfs_episode0.index = dfs_episode0.index.set_levels([episodestart], level=\"episodestart\")\n",
    "dfs_episode0.index = dfs_episode0.index.set_levels(\n",
    "    [[trucks_by_id[\"VB7\"].vid], [drivers_by_id[\"zheng-longfei\"].pid]],\n",
    "    level=[\"vehicle\", \"driver\"],\n",
    "    verify_integrity=False,\n",
    ")\n",
    "dfs_episode1 = dfs_episode.copy()\n",
    "dfs_episode1.index = dfs_episode1.index.set_levels(\n",
    "    [episodestart - pd.Timedelta(3, \"d\")], level=\"episodestart\"\n",
    ")\n",
    "dfs_episode1.index = dfs_episode1.index.set_levels(\n",
    "    [[trucks_by_id[\"MP73\"].vid], [drivers_by_id[\"wang-cheng\"].pid]],\n",
    "    level=[\"vehicle\", \"driver\"],\n",
    "    verify_integrity=False,\n",
    ")\n",
    "dfs_episode2 = dfs_episode.copy()\n",
    "dfs_episode2.index = dfs_episode2.index.set_levels(\n",
    "    [episodestart - pd.Timedelta(4, \"d\")], level=\"episodestart\"\n",
    ")\n",
    "dfs_episode2.index = dfs_episode2.index.set_levels(\n",
    "    [[trucks_by_id[\"VB7\"].vid], [drivers_by_id[\"wang-cheng\"].pid]],\n",
    "    level=[\"vehicle\", \"driver\"],\n",
    "    verify_integrity=False,\n",
    ")\n",
    "dfs_episode3 = dfs_episode.copy()\n",
    "dfs_episode3.index = dfs_episode3.index.set_levels(\n",
    "    [episodestart - pd.Timedelta(5, \"d\")], level=\"episodestart\"\n",
    ")\n",
    "dfs_episode3.index = dfs_episode3.index.set_levels(\n",
    "    [[trucks_by_id[\"MP73\"].vid], [drivers_by_id[\"zheng-longfei\"].pid]],\n",
    "    level=[\"vehicle\", \"driver\"],\n",
    "    verify_integrity=False,\n",
    ")\n",
    "dfs_episode0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs_episode_concat = pd.concat([dfs_episode, dfs_episode0], axis=0)\n",
    "# display('dfs_episode_concat')\n",
    "# dfs_episode_concat.index\n",
    "# dfs_episode_concat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "episodes = [dfs_episode, dfs_episode0, dfs_episode1, dfs_episode2, dfs_episode3]\n",
    "try:\n",
    "    dfs_episode_all = reduce(\n",
    "        lambda left, right,: pd.concat([left, right], axis=0), episodes\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "# dfs_episode_all.sort_index(inplace=True)\n",
    "dfs_episode_all = dfs_episode_all.swaplevel(1, 0, axis=0)\n",
    "dfs_episode_all = dfs_episode_all.swaplevel(1, 2, axis=0)\n",
    "dfs_episode_all.sort_index(inplace=True)\n",
    "dfs_episode_all = dfs_episode_all[[\"state\", \"action\", \"reward\", \"nstate\"]]\n",
    "display(\"dfs_episode_all\")\n",
    "dfs_episode_all.index\n",
    "dfs_episode_all.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make MultiIndex to Rows by adding levels of \"Vehicle\" and \"Driver\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  reset index to move vehicle and driver to columns, preprocessing for dask manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episodes = dfs_episode_all.reset_index(\n",
    "    level=[\"vehicle\", \"driver\"]\n",
    ")  # unstack vehicle and driver to columns in level 0 with default '' in level 1\n",
    "# # dfs_episodes.columns\n",
    "# # dfs_episode0.index = dfs_episode0.index.set_levels([[trucks[0].vid],[drivers[1].pid]], level=['vehicle', 'driver'], verify_integrity=False)\n",
    "# old_columns = dfs_episodes.columns\n",
    "# # old_columns[0]= (old_columns[0][0],0)\n",
    "# new_columns_tuples = [(old_columns[0][0],0), (old_columns[1][0],0)] + old_columns[2:].to_list()\n",
    "# prepend_columns = pd.MultiIndex.from_tuples(new_columns_tuples, names=old_columns.names)\n",
    "# # prepend_columns\n",
    "\n",
    "# dfs_episodes.columns = prepend_columns\n",
    "dfs_episodes\n",
    "# dfs_episodes.columns\n",
    "# dfs_episodes.dtypes\n",
    "# dfs_episode_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episodes.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episodes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episodes[\"vehicle\"] = dfs_episodes[\"vehicle\"].astype(\"category\")\n",
    "dfs_episodes[\"driver\"] = dfs_episodes[\"driver\"].astype(\"category\")\n",
    "# dfs_episodes.dtypes\n",
    "# dfs_episodes.columns\n",
    "# dfs_episodes[['vehicle','driver']].dtypes\n",
    "# dfs_episodes[['vehicle','driver']]\n",
    "# dfs_episodes[['vehicle','driver']].iloc[:3].dtypes\n",
    "# sliced = dfs_episodes[['vehicle']].iloc[:3].stack(level=[-1,-2])\n",
    "# type(sliced)\n",
    "# sliced.index\n",
    "# sliced.values\n",
    "\n",
    "dfs_episodes_sorted = dfs_episodes.sort_index(\n",
    "    axis=1, level=[0, 1, 2], ascending=[True, True, True]\n",
    ")\n",
    "dfs_episodes_sorted.sort_index(axis=0, inplace=True)\n",
    "# dfs_episodes_sorted = dfs_episodes.sort_index(axis=1, level=[0,1], ascending=[True, True])\n",
    "# dfs_episodes_sorted = dfs_episodes.sort_index(axis=0, level=[0,1], ascending=[True, True])\n",
    "# dfs_episodes_sorted.sort_index(axis=1,inplace=True)\n",
    "dfs_episodes_sorted\n",
    "dfs_episodes_sorted.columns\n",
    "\n",
    "# dfs_episodes[['driver'], ['vehicle']] = dfs_episodes[['driver', 'vehicle']].apply(lambda x: x.cat.codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataframe slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts\n",
    "ep_start = ts - pd.Timedelta(9, \"D\")\n",
    "ep_start\n",
    "# time_slice\n",
    "episode_slice = pd.date_range(ep_start, periods=6, freq=\"D\")\n",
    "episode_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_slice = pd.date_range(ts, periods=3, freq=\"H\") + pd.Timedelta(10, \"min\")\n",
    "timestamp_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episodes_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episodes_sorted.index\n",
    "dfs_episodes_sorted.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs_episode_all.loc['VB7', 'zheng-longfei', episode_slice[0]:episode_slice[3]]\n",
    "# dfs_selected_episodes = dfs_episodes_sorted.loc[trucks[0].vid, drivers[1].pid, episode_slice[0]:episode_slice[3], timestamp_slice[0]:timestamp_slice[2]]\n",
    "idx = pd.IndexSlice\n",
    "dfs_selected_episodes = dfs_episodes_sorted.loc[\n",
    "    idx[episode_slice[0] : episode_slice[5], timestamp_slice[0] : timestamp_slice[2]],\n",
    "    idx[\"action\":\"driver\"],\n",
    "]\n",
    "dfs_selected_episodes\n",
    "# dfs_selected_episodes[['action', 'reward']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs_selected_episodes_columns = dfs_episode_all.loc['VB7', 'zheng-longfei', episode_slice[0]:episode_slice[3], timestamp_slice[0]:timestamp_slice[2]][['action', 'reward']]\n",
    "# dfs_selected_episodes_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs_episode_all_sorted = dfs_episode_all.sort_index(axis=1)\n",
    "# dfs_episode_all_sorted = dfs_episode_all_sorted.sort_index(axis=0)\n",
    "# dfs_episode_all_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs_selected_episodes_columns = dfs_episode_all_sorted.loc[(slice('VB6'), slice('wang-cheng'), slice(None), slice(None)),:]\n",
    "# idx = pd.IndexSlice\n",
    "# dfs_episode_all_sorted.loc[(idx['VB6'], idx['wang-cheng','zheng-longfei'], slice(None), slice(None)),:]\n",
    "# dfs_selected_episodes_columns = dfs_episode_all_sorted.loc[(idx['VB7'], idx['zheng-longfei'], slice(None), slice(None)),['state','n_state']]\n",
    "# dfs_selected_episodes_columns = dfs_episodes_sorted.loc[(idx['VB7'], idx['zheng-longfei'], slice(None), slice(None)),['state','n_state']]\n",
    "# dfs_selected_episodes_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_new = pd.to_datetime((datetime.now()))\n",
    "episodestart = ts_new - pd.Timedelta(10, \"d\") - pd.Timedelta(6, \"h\")\n",
    "episodestart\n",
    "episode_slice = pd.date_range(episodestart, periods=20, freq=\"d\") + pd.Timedelta(\n",
    "    10, \"min\"\n",
    ")\n",
    "episode_slice\n",
    "ts\n",
    "time_slice = pd.date_range(ts, periods=6, freq=\"h\") - pd.Timedelta(10, \"min\")\n",
    "time_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_slice[4]: episode_slice[8]\n",
    "idx[episode_slice[4] : episode_slice[8]]\n",
    "idx[episode_slice[4] : episode_slice[8], slice(None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_slice[4]\n",
    "episode_slice[11]\n",
    "# dfs_selected_episodes = dfs_episodes_sorted.loc[idx['VB7'], slice(None), episode_slice[4]:episode_slice[11]]\n",
    "# dfs_selected_episodes\n",
    "# dfs_selected_episodes = dfs_episodes_sorted.loc[slice(None), idx['zheng-longfei'], episode_slice[4]:episode_slice[11]]\n",
    "# dfs_selected_episodes\n",
    "# dfs_selected_episodes = dfs_episode_all.loc[slice(None), idx['zheng-longfei'], episode_slice[4]:episode_slice[11], time_slice[0]:time_slice[3]]\n",
    "# dfs_selected_episodes\n",
    "# epislice = idx(episode_slice[4]:episode_slice[11])\n",
    "\n",
    "# dfs_selected_episodes = dfs_episodes_sorted.loc[idx[episode_slice[4]:episode_slice[11],slice(None)], idx[dfs_episodes_sorted['driver']==drivers[0].pid]]\n",
    "dfs_selected_episodes1 = dfs_episodes_sorted.loc[\n",
    "    dfs_episodes_sorted[\"driver\"] == drivers_by_id[\"wang-cheng\"].pid\n",
    "]\n",
    "dfs_selected_episodes1\n",
    "dfs_selected_episodes2 = dfs_episodes_sorted.loc[\n",
    "    dfs_episodes_sorted[\"driver\"] == drivers_by_id[\"wang-cheng\"].pid\n",
    "].loc[idx[episode_slice[4] : episode_slice[8]]]\n",
    "dfs_selected_episodes2\n",
    "dfs_selected_episodes3 = dfs_episodes_sorted.loc[\n",
    "    (dfs_episodes_sorted[\"driver\"] == drivers_by_id[\"wang-cheng\"].pid)\n",
    "    & (dfs_episodes_sorted[\"vehicle\"] == trucks_by_id[\"VB7\"].vid)\n",
    "].loc[idx[episode_slice[4] : episode_slice[8]]]\n",
    "dfs_selected_episodes3\n",
    "# dfs_selected_episodes31 = dfs_episodes_sorted.loc[idx[episode_slice[4]:episode_slice[8],slice(None)]].loc[ (dfs_episodes_sorted['driver']==drivers[0].pid) & (dfs_episodes_sorted['vehicle']==trucks[0].vid)]\n",
    "# dfs_selected_episodes31 = dfs_episodes_sorted.loc[ (dfs_episodes_sorted['driver']==drivers[0].pid) & (dfs_episodes_sorted['vehicle']==trucks[0].vid) & (idx[episode_slice[4]:episode_slice[8],slice(None)])]\n",
    "# dfs_selected_episodes31\n",
    "# dfs_selected_episodes3.index\n",
    "# dfs_selected_episodes3.columns\n",
    "# dfs_selected_episodes = dfs_episodes_sorted.loc[slice(None), idx['zheng-longfei'], episode_slice[4]:episode_slice[11]]\n",
    "# dfs_selected_episodes\n",
    "# dfs_selected_episodes = dfs_episode_all.loc[slice(None), idx['zheng-longfei'], episode_slice[4]:episode_slice[11], time_slice[0]:time_slice[3]]\n",
    "# dfs_selected_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx[time_slice[0] : time_slice[3]]\n",
    "dfs_selected_episodes3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_slice2 = pd.date_range(ts, periods=6, freq=\"S\")\n",
    "time_slice2\n",
    "dfs_selected_episodes3_sorted = dfs_selected_episodes3.sort_index(axis=0).sort_index(\n",
    "    axis=1\n",
    ")[[\"action\"]]\n",
    "dfs_selected_episodes3_sorted\n",
    "# dfs_selected_episodes3.loc[:, idx[:, :, 0:2, :]]\n",
    "# dfs_selected_episodes3_sorted.loc[:, idx[:, :,time_slice2[0]:time_slice2[1], :]]\n",
    "# dfs_selected_episodes3.loc[idx[:,time_slice[0]:time_slice[2]],:]\n",
    "dfs_selected_episodes3.loc[idx[:, time_slice[0] : time_slice[3]], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_selected_episodes3.index\n",
    "# dfs_selected_episodes3.columns\n",
    "time_slice[0]\n",
    "time_slice[3]\n",
    "idx[:, time_slice[0] : time_slice[3]]\n",
    "dfs_selected_episodes4 = dfs_selected_episodes3.loc[\n",
    "    idx[:, time_slice[0] : time_slice[3]], :\n",
    "]\n",
    "# dfs_selected_episodes4 = dfs_selected_episodes3.loc[idx[:,time_slice[0]:time_slice[3],:],:]\n",
    "\n",
    "dfs_selected_episodes4\n",
    "dfs_selected_episodes41 = dfs_selected_episodes3.loc[\n",
    "    idx[:, time_slice[0] : time_slice[3]], \"action\":\"driver\"\n",
    "]\n",
    "dfs_selected_episodes41\n",
    "# dfs_selected_episodes4 = dfs_episodes_sorted.loc[ (dfs_episodes_sorted['driver']==drivers[0].pid) & (dfs_episodes_sorted['vehicle']==trucks[0].vid)].loc[idx[episode_slice[4]:episode_slice[8],time_slice[0]:time_slice[3]]]\n",
    "# dfs_selected_episodes4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs_selected_episodes = dfs_episode_all.loc['VB7', 'zheng-longfei', episode_slice[0]:episode_slice[3], timestamp_slice[0]:timestamp_slice[2]]\n",
    "# dfs_selected_episodes_columns = dfs_episode_all_sorted.loc[(idx['VB7'], idx['zheng-longfei'], slice(None), slice(None)),['state','n_state']]\n",
    "# dfs_selected_episodes_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episodes_sorted\n",
    "# dfs_episode.loc[:,'state']\n",
    "dfs_slice = dfs_episodes_sorted.loc[\n",
    "    dfs_episodes_sorted[\"vehicle\"] == \"MP73\", [\"vehicle\", \"driver\", \"state\", \"reward\"]\n",
    "]\n",
    "dfs_slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saved episodes as parquet in multi-level folders \"vehicle/driver/episode/tuple.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import platform\n",
    "\n",
    "print(\"Python: \", platform.python_version())\n",
    "print(\"pandas: \", pd.__version__)\n",
    "print(\"pyarrow: \", pa.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episodes_sorted\n",
    "dfs_episodes_sorted.index\n",
    "dfs_episodes_sorted.columns\n",
    "dfs_episodes_sorted.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_complete_episodes = dfs_episodes_sorted.reset_index(\"episodestart\")\n",
    "dfs_complete_episodes\n",
    "dfs_complete_episodes.index\n",
    "dfs_complete_episodes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs_episode4dask = dfs_episode_all_sorted.swaplevel(0,-1, axis=0).swaplevel(1,3, axis=0).swaplevel(2,3, axis=0).unstack()\n",
    "# dfs_episode4dask = dfs_episode4dask.swaplevel(0,-1, axis=1).swaplevel(-2,-1, axis=1)\n",
    "# dfs_episode4dask\n",
    "# # dfs_episode4dask = dfs_episode4dask.swaplevel(0,-1, axis=1).swaplevel(-2,-1, axis=1)\n",
    "# # dfs_episode4dask.index\n",
    "# # dfs_episode4dask.index\n",
    "# # dfs_episode4dask.columns\n",
    "# # dfs_episode4dask\n",
    "# # dfs_episode_all_stack = dfs_episode_all.unstack(level=[-1,-2,-3,])\n",
    "# # dfs_episode_all_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs_episode4dask1 = dfs_episode4dask.unstack().swaplevel(-2,-1, axis=1).swaplevel(-2,-3,axis=1).swaplevel(-3,-4,axis=1)\n",
    "# # dfs_episode4dask1\n",
    "# dfs_episode4dask2 = dfs_episode4dask1.unstack().swaplevel(-2,-1, axis=1).swaplevel(-2,-3,axis=1).swaplevel(-3,-4,axis=1).swaplevel(-4,-5,axis=1)\n",
    "# dfs_episode4dask2.sort_index(axis=1, inplace=True)\n",
    "# dfs_episode4dask2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoding multiindex dataframe to flat index for Dask DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_complete_episodes.columns\n",
    "cols = dfs_complete_episodes.columns.to_flat_index()\n",
    "cols\n",
    "\n",
    "## 3 level index\n",
    "# cols_str = [\n",
    "#     f'{x[0]}_{x[1]}_{x[2]}'\n",
    "#     if (x[1] != '')\n",
    "#     else f'{x[0]}__{x[2]}'\n",
    "#     if (x[2] != '')\n",
    "#     else f'{x[0]}'\n",
    "#     for x in cols\n",
    "# ]\n",
    "\n",
    "## 4 level index\n",
    "cols_str = [\n",
    "    f\"{x[0]}_{x[1]}_{x[2]}\"\n",
    "    if (x[1] != \"\" and x[2] != \"\")\n",
    "    else f\"{x[0]}_{x[1]}_\"\n",
    "    if (x[1] != \"\" and x[2] == \"\")\n",
    "    else f\"{x[0]}__{x[2]}\"\n",
    "    if (x[2] != \"\")\n",
    "    else f\"{x[0]}__\"\n",
    "    for x in cols\n",
    "]\n",
    "cols_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding index and columns\n",
    "dfs_complete_episodes_flat = dfs_complete_episodes.copy()\n",
    "cols = dfs_complete_episodes.columns.to_flat_index()\n",
    "dfs_complete_episodes_flat.columns = [\n",
    "    f\"{x[0]}_{x[1]}_{x[2]}\"\n",
    "    if (x[1] != \"\" and x[2] != \"\")\n",
    "    else f\"{x[0]}_{x[1]}_\"\n",
    "    if (x[1] != \"\" and x[2] == \"\")\n",
    "    else f\"{x[0]}__{x[2]}\"\n",
    "    if (x[2] != \"\")\n",
    "    else f\"{x[0]}__\"\n",
    "    for x in cols\n",
    "]\n",
    "dfs_complete_episodes_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_complete_episodes_flat\n",
    "dfs_complete_episodes_flat.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_complete_episodes_flat[\"vehicle__\"].iloc[0:2].values.dtype\n",
    "dfs_complete_episodes_flat[\"vehicle__\"].dtype\n",
    "dfs_complete_episodes_flat[\"state_velocity_0\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ddf_episodes = dd.from_pandas(dfs_complete_episodes_flat, npartitions=1)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    ddf_episodes.index\n",
    "    ddf_episodes.columns\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "ddf_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_metadata = {\n",
    "    b\"eos\": b'{\"length\":[3, 600, 5],\"timezeone\":\"sh\",\"units\":[\"kph\",\"pct\",\"pct\"],\"data_propensity\":[50, 1, 4],\"Dataset Description\": \"MP vehicle from TBox\"}'\n",
    "}\n",
    "custom_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with ProgressBar():\n",
    "        ddf_episodes.to_parquet(\n",
    "            \"eos_complete_episodes\",\n",
    "            engine=\"pyarrow\",\n",
    "            compression=\"snappy\",\n",
    "            partition_on=[\"vehicle__\", \"driver__\", \"episodestart__\"],\n",
    "            write_metadata_file=True,\n",
    "            custom_metadata=custom_metadata,\n",
    "        )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read saved parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_parquet(\n",
    "    \"eos_complete_episodes\",\n",
    "    engine=\"pyarrow\",\n",
    "    compression=\"snappy\",\n",
    "    ignore_metadata_file=False,\n",
    "    infer_divisions=True,\n",
    ")\n",
    "ddf.head(5)\n",
    "ddf.dtypes\n",
    "ddf.npartitions\n",
    "ddf.divisions\n",
    "ddf._meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get customized metadata with pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "\n",
    "# print('pyarrow: ', pa.__version__)\n",
    "\n",
    "# table = pq.read_table('ddf_episodes')\n",
    "# table_meta = table.schema.metadata[b'eos']\n",
    "# print(table_meta)\n",
    "# # custom_metadata_info = json.loads(table.schema.metadata[b'eos'])\n",
    "# # print(custom_metadata_info)\n",
    "# try:\n",
    "#     custom_meta_info = json.loads(table_meta)\n",
    "# except Exception as e:\n",
    "#     print(f'Exception: {e}')\n",
    "\n",
    "# print(custom_meta_info)\n",
    "\n",
    "# # Print formatted output of the dichtionary custom_meta_info:\n",
    "# for key, val in custom_meta_info.items():\n",
    "#     print('{:15}: {}'.format(key, val))\n",
    "\n",
    "# ALTERNATE:\n",
    "\n",
    "table_meta = pq.read_metadata(\"eos_complete_episodes/_common_metadata\")\n",
    "# print(table_meta)\n",
    "# print(table_meta.metadata[b'eos'])\n",
    "custom_meta_info = table_meta.metadata[b\"eos\"]\n",
    "custom_meta_info = json.loads(custom_meta_info)\n",
    "# custom_meta_info\n",
    "\n",
    "for key, val in custom_meta_info.items():\n",
    "    print(f\"{key}: {val}\")\n",
    "\n",
    "# table_meta = pq.read_metadata('ddf_episodes/_metadata')\n",
    "# # print(table_meta)\n",
    "# # print(table_meta.metadata[b'eos'])\n",
    "# custom_meta_info = table_meta.metadata[b'eos']\n",
    "# custom_meta_info = json.loads(custom_meta_info)\n",
    "# custom_meta_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoding flat_index of dask dataframe to multiindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_episodes_read = ddf.compute()\n",
    "df_episodes_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoding\n",
    "multi_cols = [tuple(col.split(\"_\")) for col in df_episodes_read.columns]\n",
    "multi_cols\n",
    "# multi_cols = [(col[0], int(col[1])) if len(col)==2 else col for col in multi_cols]\n",
    "# multi_cols\n",
    "df_episodes_read_multicol = df_episodes_read.copy()\n",
    "multi_idx = pd.MultiIndex.from_tuples(multi_cols)\n",
    "# multi_idx\n",
    "multi_idx.names = [\"tuple\", \"rows\", \"idx\"]\n",
    "driver_vehicle = [\n",
    "    idx\n",
    "    for idx in multi_idx\n",
    "    if idx[0] == \"driver\" or idx[0] == \"vehicle\" or idx[0] == \"episodestart\"\n",
    "]\n",
    "driver_vehicle\n",
    "# multi_idx = multi_idx.set_levels([multi_idx.levels[0], multi_idx.levels[1].astype(int)])\n",
    "# multi_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series([1, np.nan,2,None])\n",
    "# pd.Series([1,np.nan,2,None,pd.NA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoding columns\n",
    "from datetime import datetime\n",
    "\n",
    "multi_tpl = [tuple(col.split(\"_\")) for col in df_episodes_read.columns]\n",
    "# multi_tpl\n",
    "# multi_cols = [(col[0], int(col[1])) if len(col)==2 else col for col in multi_cols]\n",
    "# multi_cols\n",
    "df_episodes_read_multicol = df_episodes_read.copy()\n",
    "multi_col = pd.MultiIndex.from_tuples(multi_tpl)\n",
    "# multi_idx\n",
    "# multi_col\n",
    "i1 = multi_col.get_level_values(0)\n",
    "i1 = [\n",
    "    \"\" if str(idx) in (str(pd.NA), \"nan\", \"\") else idx for idx in i1\n",
    "]  # convert index of level 2 type to int and '' if NA\n",
    "i1\n",
    "# i2 = multi_col.get_level_values(1).fillna('') # must be null string instead of the default pd.NA or np.nan\n",
    "# i2 = [idx if isinstance(idx, int) else '' for idx in i2]\n",
    "i2 = multi_col.get_level_values(\n",
    "    1\n",
    ")  # must be null string instead of the default pd.NA or np.nan\n",
    "i2 = [\n",
    "    \"\" if str(idx) in (str(pd.NA), \"nan\", \"\") else idx for idx in i2\n",
    "]  # convert index of level 2 type to int and '' if NA\n",
    "i2\n",
    "# i2 = [idx if isinstance(idx, str) else '' for idx in i2]\n",
    "# i2.astype('int')\n",
    "i3 = multi_col.get_level_values(\n",
    "    2\n",
    ")  # must be null string instead of the default pd.NA or np.nan\n",
    "i3 = [\n",
    "    \"\" if str(idx) in (str(pd.NA), \"nan\", \"\") else int(idx) for idx in i3\n",
    "]  # convert index of level 2 type to int and '' if NA\n",
    "i3\n",
    "\n",
    "\n",
    "multi_col = pd.MultiIndex.from_arrays([i1, i2, i3])\n",
    "multi_col.names = [\"tuple\", \"rows\", \"idx\"]\n",
    "multi_col\n",
    "\n",
    "\n",
    "# i2[-1]\n",
    "# i2.dropna()\n",
    "# i2\n",
    "# l2 = ['' if i==nan else i  for i in i2]\n",
    "# l2\n",
    "# i2= i2[:-2].astype('int')\n",
    "# i2 = i2.append(pd.Index(['','']))\n",
    "# l2 = [''] + list(l2.values[:-1])\n",
    "# print(l2)\n",
    "# arrays = [ for i2 in l2 for i1 in l1]\n",
    "# l2 = l2[:-2].astype('int') + ['', '']\n",
    "# multi_col.set_levels(ll, level=['tuple','step'])\n",
    "# driver_vehicle = [idx for idx in multi_idx if idx[0]=='driver' or idx[0]=='vehicle']\n",
    "# multi_tpl =  [(col[0],)  if col[0]=='driver' or col[0]=='vehicle' else (col[0],int(col[1])) for col in multi_col]\n",
    "# multi_idx1 = pd.MultiIndex.from_tuples(multi_idx1)\n",
    "# multi_idx = multi_idx.set_levels([multi_idx.levels[0], multi_idx.levels[1].astype(int)])\n",
    "# multi_col = pd.MultiIndex.from_tuples(multi_tpl)\n",
    "# multi_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # decoding index\n",
    "# multi_tuple = [tuple(idx.split('_')) for idx in df_episodes_read.index]\n",
    "# multi_idx = pd.MultiIndex.from_tuples(multi_tuple)\n",
    "# # multi_idx\n",
    "# i0 = multi_idx.get_level_values(\n",
    "#     0\n",
    "# )  # must be null string instead of the default pd.NA or np.nan\n",
    "# i0 = [\n",
    "#     ''\n",
    "#     if str(idx) in (str(pd.NA), 'nan', '')\n",
    "#     else datetime.strptime(idx, '%Y-%m-%dT%H:%M:%S.%f')\n",
    "#     for idx in i0\n",
    "# ]  # convert index of level 2 type to int and '' if NA\n",
    "# i1 = multi_idx.get_level_values(\n",
    "#     1\n",
    "# )  # must be null string instead of the default pd.NA or np.nan\n",
    "# i1 = [\n",
    "#     ''\n",
    "#     if str(idx) in (str(pd.NA), 'nan', '')\n",
    "#     else datetime.strptime(idx, '%Y-%m-%dT%H:%M:%S.%f')\n",
    "#     for idx in i1\n",
    "# ]  # convert index of level 2 type to int and '' if NA\n",
    "# df_episodes_read_multiidx = df_episodes_read.copy()\n",
    "# multi_idx = pd.MultiIndex.from_arrays([i0, i1], names=('episodestart', 'timestamp'))\n",
    "#\n",
    "# multi_idx.dtypes\n",
    "# multi_idx\n",
    "# len(multi_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_episodes_read_multi = df_episodes_read.copy()\n",
    "df_episodes_read_multi.columns = multi_col\n",
    "# df_episodes_read_multi.index = multi_idx\n",
    "df_episodes_read_multi\n",
    "df_episodes_read_multi.columns\n",
    "df_episodes_read_multi.index\n",
    "df_episodes_read_multi.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recover deep multiindex from columns ['vehicle','driver','episodestart']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_episodes_read_multi_setindex = df_episodes_read_multi.set_index(\n",
    "    [\"vehicle\", \"driver\", \"episodestart\", df_episodes_read_multi.index]\n",
    ")\n",
    "# df_episodes_read_multi_setindex\n",
    "# df_episodes_read_multi_setindex.sort_index(inplace=True)\n",
    "df_episodes_read_multi_setindex = df_episodes_read_multi_setindex.swaplevel(\n",
    "    1, 2, axis=0\n",
    ").swaplevel(0, 1, axis=0)\n",
    "# df_episodes_read_multi_setindex.index\n",
    "# df_episodes_read_multi_setindex.columns\n",
    "df_episodes_read_multi_setindex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## append new episode to the specific parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episode4 = dfs_episode.copy()\n",
    "dfs_episode4.index = dfs_episode3.index.set_levels(\n",
    "    [episodestart - pd.Timedelta(8, \"d\")], level=\"episodestart\"\n",
    ")\n",
    "dfs_episode4.index = dfs_episode3.index.set_levels(\n",
    "    [[trucks_by_id[\"VB4\"].vid], [drivers_by_id[\"zheng-longfei\"].pid]],\n",
    "    level=[\"vehicle\", \"driver\"],\n",
    "    verify_integrity=False,\n",
    ")\n",
    "# dfs_episode4\n",
    "dfs_episode4 = dfs_episode4.swaplevel(1, 2, axis=0).swaplevel(0, 1, axis=0)\n",
    "dfs_episode4\n",
    "\n",
    "# dfs_episode5 = dfs_episode.copy()\n",
    "# dfs_episode5.index = dfs_episode3.index.set_levels([episodestart-pd.Timedelta(9,'d')], level='episode')\n",
    "# df_episodes_read_multi_setindex.columns\n",
    "# df_episodes_read_multi_setindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episodes_new = pd.concat([df_episodes_read_multi_setindex, dfs_episode4], axis=0)\n",
    "dfs_episodes_new.index.names = [\"episodestart\", \"vehicle\", \"driver\", \"timestamp\"]\n",
    "dfs_episodes_new\n",
    "dfs_episodes_new.columns\n",
    "dfs_episodes_new.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save new dataframe back to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episodes_new_flat = dfs_episodes_new.copy()\n",
    "dfs_episodes_new_flat = dfs_episodes_new_flat.reset_index(\n",
    "    level=[\"vehicle\", \"driver\", \"episodestart\"]\n",
    ")\n",
    "dfs_episodes_new_flat.columns\n",
    "dfs_episodes_new_flat.index\n",
    "dfs_episodes_new_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding index\n",
    "# from datetime import datetime\n",
    "\n",
    "# flat_idx = dfs_episodes_new_flat.index.to_flat_index()\n",
    "# flat_idx = [\n",
    "#     f'{x[0].strftime(\"%Y-%m-%dT%H:%M:%S.%f\")}_{x[1].strftime(\"%Y-%m-%dT%H:%M:%S.%f\")}'\n",
    "#     if x[1] != ''\n",
    "#     else f'{x[0].strftime(\"%Y-%m-%dT%H:%M:%S.%f\")}'\n",
    "#     for x in flat_idx\n",
    "# ]\n",
    "# # flat_idx\n",
    "\n",
    "# encoding columns\n",
    "flat_cols = dfs_episodes_new_flat.columns.to_flat_index()\n",
    "# flat_cols\n",
    "# flat_cols = [\n",
    "#     f'{x[0]}_{x[1]}_{x[2].strftime(\"%Y-%m-%dT%H:%M:%S.%f\")}_{x[3]}'\n",
    "#     if (x[1] != '' and x[3] not in ('', pd.NaT, str(pd.NA)))\n",
    "#     else f'{x[0]}_{x[1]}_{x[2].strftime(\"%Y-%m-%dT%H:%M:%S.%f\")}_'\n",
    "#     if (x[1] != '' and x[3] in ('', pd.NaT, str(pd.NA)))\n",
    "#     else f'{x[0]}__{x[2].strftime(\"%Y-%m-%dT%H:%M:%S.%f\")}_'\n",
    "#     if (x[2] not in ('', pd.NaT, str(pd.NA)))\n",
    "#     else f'{x[0]}___'\n",
    "#     for x in flat_cols\n",
    "# ]\n",
    "flat_cols = [\n",
    "    f\"{x[0]}_{x[1]}_{x[2]}\"\n",
    "    if (x[1] != \"\" and x[2] not in (\"\", pd.NaT, str(pd.NA)))\n",
    "    else f\"{x[0]}_{x[1]}_\"\n",
    "    if (x[1] != \"\" and x[2] in (\"\", pd.NaT, str(pd.NA)))\n",
    "    else f\"{x[0]}__{x[2]}\"\n",
    "    if (x[2] not in (\"\", pd.NaT, str(pd.NA)))\n",
    "    else f\"{x[0]}__\"\n",
    "    for x in flat_cols\n",
    "]\n",
    "flat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episodes_new_flat.columns = flat_cols\n",
    "# dfs_episodes_new_flat.index = flat_idx\n",
    "dfs_episodes_new_flat[\"vehicle__\"] = dfs_episodes_new_flat[\"vehicle__\"].astype(\n",
    "    \"category\"\n",
    ")\n",
    "dfs_episodes_new_flat[\"driver__\"] = dfs_episodes_new_flat[\"driver__\"].astype(\"category\")\n",
    "dfs_episodes_new_flat[\"episodestart__\"] = dfs_episodes_new_flat[\n",
    "    \"episodestart__\"\n",
    "].astype(\"datetime64[ns]\")\n",
    "# dfs_episodes_new_flat.index.name = 'episodestart_timestamp'\n",
    "dfs_episodes_new_flat.index\n",
    "dfs_episodes_new_flat.dtypes\n",
    "# dfs_episodes_new_flat.columns.name = 'episodestart_timestamp'\n",
    "dfs_episodes_new_flat.columns\n",
    "dfs_episodes_new_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ddf_episodes_new = dd.from_pandas(dfs_episodes_new_flat, npartitions=1)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "ddf_episodes_new.dtypes\n",
    "# dfs_episodes_new_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "custom_metadata = {\n",
    "    b\"eos\": b'{\"length\":[6, 600, 5],\"timezeone\":\"sh\",\"units\":[\"kph\",\"pct\",\"pct\"],\"data_propensity\":[50, 1, 4],\"Dataset Description\": \"MP vehicle from TBox\"}'\n",
    "}\n",
    "\n",
    "try:\n",
    "    with ProgressBar():\n",
    "        ddf_episodes_new.to_parquet(\n",
    "            \"eos_complete_episodes\",\n",
    "            engine=\"pyarrow\",\n",
    "            compression=\"snappy\",\n",
    "            partition_on=[\"vehicle__\", \"driver__\", \"episodestart__\"],\n",
    "            write_metadata_file=True,\n",
    "            custom_metadata=custom_metadata,\n",
    "        )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_parquet(\n",
    "    \"eos_complete_episodes\",\n",
    "    engine=\"pyarrow\",\n",
    "    compression=\"snappy\",\n",
    "    ignore_metadata_file=False,\n",
    "    infer_divisions=True,\n",
    ")\n",
    "pq_meta = pq.read_metadata(\"eos_episodes/_common_metadata\")\n",
    "custom_meta_info = pq_meta.metadata[b\"eos\"]\n",
    "custom_meta_info = json.loads(custom_meta_info)\n",
    "for key, val in custom_meta_info.items():\n",
    "    print(f\"{key}: {val}\")\n",
    "ddf.dtypes\n",
    "df = ddf.compute()\n",
    "\n",
    "# decoding columns index\n",
    "multi_tpl = [tuple(col.split(\"_\")) for col in df.columns]\n",
    "multi_col = pd.MultiIndex.from_tuples(multi_tpl)\n",
    "i1 = multi_col.get_level_values(0)\n",
    "i1 = [\"\" if str(idx) in (str(pd.NA), \"nan\", \"\") else idx for idx in i1]\n",
    "i2 = multi_col.get_level_values(\n",
    "    1\n",
    ")  # must be null string instead of the default pd.NA or np.nan\n",
    "i2 = [\n",
    "    \"\" if str(idx) in (str(pd.NA), \"nan\", \"\") else idx for idx in i2\n",
    "]  # convert index of level 2 type to int and '' if NA\n",
    "i3 = multi_col.get_level_values(\n",
    "    2\n",
    ")  # must be null string instead of the default pd.NA or np.nan\n",
    "i3 = [\n",
    "    \"\" if str(idx) in (str(pd.NA), \"nan\", \"\") else int(idx) for idx in i3\n",
    "]  # convert index of level 2 type to int and '' if NA\n",
    "\n",
    "\n",
    "multi_col = pd.MultiIndex.from_arrays([i1, i2, i3])\n",
    "multi_col.names = [\"tuple\", \"rows\", \"idx\"]\n",
    "multi_col\n",
    "\n",
    "# # decoding index\n",
    "# multi_tuple = [tuple(idx.split('_')) for idx in df.index]\n",
    "# multi_idx = pd.MultiIndex.from_tuples(multi_tuple)\n",
    "\n",
    "# i0 = multi_idx.get_level_values(\n",
    "#     0\n",
    "# )  # must be null string instead of the default pd.NA or np.nan\n",
    "# i0 = [\n",
    "#     ''\n",
    "#     if str(idx) in (str(pd.NA), 'nan', '')\n",
    "#     else datetime.strptime(idx, '%Y-%m-%dT%H:%M:%S.%f')\n",
    "#     for idx in i0\n",
    "# ]  # convert index of level 2 type to int and '' if NA\n",
    "# i1 = multi_idx.get_level_values(\n",
    "#     1\n",
    "# )  # must be null string instead of the default pd.NA or np.nan\n",
    "# i1 = [\n",
    "#     ''\n",
    "#     if str(idx) in (str(pd.NA), 'nan', '')\n",
    "#     else datetime.strptime(idx, '%Y-%m-%dT%H:%M:%S.%f')\n",
    "#     for idx in i1\n",
    "# ]  # convert index of level 2 type to int and '' if NA\n",
    "# df_episodes_read_multiidx = df_episodes_read.copy()\n",
    "# multi_idx = pd.MultiIndex.from_arrays([i0, i1], names=('episodestart', 'timestamp'))\n",
    "#\n",
    "# multi_idx.names = ['episodestart', 'timestamp']\n",
    "\n",
    "df.columns = multi_col\n",
    "# df.index = multi_idx\n",
    "\n",
    "df = df.set_index([\"vehicle\", \"driver\", \"episodestart\", df.index])\n",
    "df.index.dtypes\n",
    "df.index\n",
    "# len(multi_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add rows to a partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_parquet(\n",
    "    \"eos_complete_episodes\",\n",
    "    engine=\"pyarrow\",\n",
    "    compression=\"snappy\",\n",
    "    ignore_metadata_file=False,\n",
    "    infer_divisions=True,\n",
    ")\n",
    "ddf.index\n",
    "ddf.npartitions\n",
    "ddf.divisions\n",
    "ddf.dtypes\n",
    "\n",
    "# ddf['episodestart'] = ddf['episodestart'].astype('datetime64[ns]')  # error,  not allowed for dask dataframe\n",
    "# ddf.assign(episodestart=ddf['episodestart'].astype('datetime64[ns]'))\n",
    "ddf = ddf[\"episodestart\"].astype(\"datetime64[ns]\")\n",
    "ddf.dtypes\n",
    "# ddf.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episode5 = dfs_episode4.copy()\n",
    "dfs_episode5.index = dfs_episode4.index.set_levels(\n",
    "    [episodestart - pd.Timedelta(9, \"d\")], level=\"episodestart\"\n",
    ")\n",
    "dfs_episode5.index = dfs_episode4.index.set_levels(\n",
    "    [[trucks_by_id[\"MP74\"].vid], [drivers_by_id[\"zheng-longfei\"].pid]],\n",
    "    level=[\"vehicle\", \"driver\"],\n",
    "    verify_integrity=False,\n",
    ")\n",
    "# dfs_episode5 = dfs_episode5.swaplevel(0,1, axis=0).swaplevel(1,2,axis=0)\n",
    "dfs_episode5 = dfs_episode5.reset_index(level=[\"vehicle\", \"driver\", \"episodestart\"])\n",
    "\n",
    "dfs_episode5[\"vehicle\"] = dfs_episode5[\"vehicle\"].astype(\"category\")\n",
    "dfs_episode5[\"driver\"] = dfs_episode5[\"driver\"].astype(\"category\")\n",
    "# dfs_episode5['episodestart'] = dfs_episode5['episodestart'].astype('category')\n",
    "dfs_episode5.dtypes\n",
    "\n",
    "\n",
    "# encoding columns index\n",
    "dfs_episode5.columns = [\n",
    "    f\"{x[0]}_{x[1]}_{x[2]}\"\n",
    "    if (x[1] != \"\" and x[2] != \"\")\n",
    "    else f\"{x[0]}_{x[1]}_\"\n",
    "    if (x[1] != \"\" and x[2] == \"\")\n",
    "    else f\"{x[0]}__{x[2]}\"  # !!! dunder!!!\n",
    "    if (x[2] != \"\")\n",
    "    else f\"{x[0]}__\"\n",
    "    for x in dfs_episode5.columns.to_flat_index()\n",
    "]\n",
    "# df_episode_new.index = [\n",
    "#     f'{x[0].strftime(\"%Y-%m-%dT%H:%M:%S.%f\")}_{x[1].strftime(\"%Y-%m-%dT%H:%M:%S.%f\")}'\n",
    "#     if x[1] != ''\n",
    "#     else f'{x[0].strftime(\"%Y-%m-%dT%H:%M:%S.%f\")}'\n",
    "#     for x in dfs_episode5.index.to_flat_index()\n",
    "# ]\n",
    "# df_episode_new.dtypes\n",
    "dfs_episode5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episode6 = dfs_episode4.copy()\n",
    "dfs_episode6.index = dfs_episode4.index.set_levels(\n",
    "    [episodestart - pd.Timedelta(19, \"d\")], level=\"episodestart\"\n",
    ")\n",
    "dfs_episode6.index = dfs_episode4.index.set_levels(\n",
    "    [[trucks_by_id[\"MP73\"].vid], [drivers_by_id[\"wang-cheng\"].pid]],\n",
    "    level=[\"vehicle\", \"driver\"],\n",
    "    verify_integrity=False,\n",
    ")\n",
    "# dfs_episode6 = dfs_episode5.swaplevel(0, 1, axis=0).swaplevel(1, 2, axis=0)\n",
    "dfs_episode6 = dfs_episode6.reset_index(level=[\"vehicle\", \"driver\", \"episodestart\"])\n",
    "\n",
    "dfs_episode6[\"vehicle\"] = dfs_episode6[\"vehicle\"].astype(\"category\")\n",
    "dfs_episode6[\"driver\"] = dfs_episode6[\"driver\"].astype(\"category\")\n",
    "# dfs_episode6['episodestart'] = dfs_episode6['episodestart'].astype('category')\n",
    "dfs_episode6.dtypes\n",
    "# encoding columns index\n",
    "dfs_episode6.columns = [\n",
    "    f\"{x[0]}_{x[1]}_{x[2]}\"\n",
    "    if (x[1] != \"\" and x[2] != \"\")\n",
    "    else f\"{x[0]}_{x[1]}_\"\n",
    "    if (x[1] != \"\" and x[2] == \"\")\n",
    "    else f\"{x[0]}__{x[2]}\"  # !!! dunder!!!\n",
    "    if (x[2] != \"\")\n",
    "    else f\"{x[0]}__\"\n",
    "    for x in dfs_episode6.columns.to_flat_index()\n",
    "]\n",
    "# dfs_episode6.index = [\n",
    "#     f'{x[0].strftime(\"%Y-%m-%dT%H:%M:%S.%f\")}_{x[1].strftime(\"%Y-%m-%dT%H:%M:%S.%f\")}'\n",
    "#     if x[1] != ''\n",
    "#     else f'{x[0].strftime(\"%Y-%m-%dT%H:%M:%S.%f\")}'\n",
    "#     for x in dfs_episode6.index.to_flat_index()\n",
    "# ]\n",
    "# df_episode_new.dtypes\n",
    "dfs_episode6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### encoding single dataframe of multiindex into dask single index dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_read = dd.read_parquet(\n",
    "    \"eos_complete_episodes\",\n",
    "    engine=\"pyarrow\",\n",
    "    compression=\"snappy\",\n",
    "    ignore_metadata_file=False,\n",
    "    infer_divisions=True,\n",
    ")\n",
    "# df_all = ddf_read.compute()\n",
    "# df_all\n",
    "# idx = pd.IndexSlice\n",
    "# col_epistart = ddf_read.loc[:, 'episodestart__'].astype('datetime64[ns]').compute()\n",
    "# col_epistart\n",
    "#\n",
    "# ddf_read.loc[:, 'episodestart__'] = ddf_read.loc[:, 'episodestart__'].astype(\n",
    "#     'datetime64[ns]'\n",
    "# )\n",
    "ddf_read[\"episodestart__\"] = ddf_read[\"episodestart__\"].astype(\"datetime64[ns]\")\n",
    "ddf_read.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddf_episodes_new = dd.concat([dfs_episode5, dfs_episode6])\n",
    "# ddf_combine = dd.concat([ddf_read, ddf_episodes_new])\n",
    "ddf_combine = dd.concat([ddf_read, dfs_episode5, dfs_episode6])\n",
    "ddf_combine.compute()\n",
    "ddf_combine.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddf_list = []\n",
    "# for n in range(ddf_read.npartitions):\n",
    "#     ddf_list.append(ddf_read.partitions[n])\n",
    "#     # ddf_list[n].compute()\n",
    "# len(ddf_list)\n",
    "# type(ddf_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new_list = [dfs_episode5, dfs_episode6]\n",
    "# df_attach = []\n",
    "# for df in df_new_list:\n",
    "#     bHit = False\n",
    "#     for i, ddf in enumerate(ddf_list):\n",
    "#         # if dfs_episode_new['vehicle']\n",
    "#         print(f\"{i}\")\n",
    "#         # df = ddf.compute()\n",
    "#         # ddf.columns\n",
    "#         # ddf.dtypes\n",
    "#         # type(ddf)\n",
    "#         vehicle = ddf['vehicle__'].compute()[0]\n",
    "#         driver = ddf['driver__'].compute()[0]\n",
    "#         # df_episode_new = df_episode_new.append(ddf)\n",
    "#         if df['vehicle__'][0] == vehicle and df['driver__'][0] == driver:\n",
    "#             bHit = True\n",
    "#             print(\n",
    "#                 f'hit {i}{\"st\" if i==1 else \"nd\" if i==2 else \"rd\" if i==3 else \"th\"} partition'\n",
    "#             )\n",
    "#             # ddf_list[i] = dd.concat([ddf, df], axis=0)\n",
    "#             df_attach.append(df)\n",
    "#             # append ddf to dfs_episode_new\n",
    "#\n",
    "#     if bHit == False:\n",
    "#         print('no hit')\n",
    "#         df_attach.append(df)\n",
    "#         # append ddf to dfs_episode_new\n",
    "#\n",
    "#\n",
    "# #     c_ep = df_episode_new['ep\n",
    "#     dfs_episode_new = dfs_episode_newappend(ddf)\n",
    "# print(f\"splicing dfs_episode_new {c_ep} with\n",
    "# df_attach.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_attach[0].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddf_list.append(df_attach[0])\n",
    "# ddf_list[0].dtypes\n",
    "# ddf_all = dd.concat(ddf_list, axis=0, join='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddf_all.compute()os.getcwd()\n",
    "custom_metadata = {\n",
    "    b\"eos\": b'{\"length\":[8, 800, 8],\"timezeone\":\"sh\",\"units\":[\"kph\",\"pct\",\"pct\"],\"data_propensity\":[50, 1, 4],\"Dataset Description\": \"MP vehicle from TBox\"}'\n",
    "}\n",
    "\n",
    "try:\n",
    "    with ProgressBar():\n",
    "        ddf_combine.to_parquet(\n",
    "            \"eos_combine_episodes\",\n",
    "            engine=\"pyarrow\",\n",
    "            compression=\"snappy\",\n",
    "            partition_on=[\"vehicle__\", \"driver__\", \"episodestart__\"],\n",
    "            write_metadata_file=True,\n",
    "            custom_metadata=custom_metadata,\n",
    "        )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddf_all.compute()\n",
    "ddf = dd.read_parquet(\n",
    "    \"eos_combine_episodes\",\n",
    "    engine=\"pyarrow\",\n",
    "    compression=\"snappy\",\n",
    "    ignore_metadata_file=False,\n",
    "    infer_divisions=True,\n",
    ")\n",
    "pq_meta = pq.read_metadata(\"eos_combine_episodes/_common_metadata\")\n",
    "# print(table_meta)\n",
    "# print(table_meta.metadata[b'eos'])\n",
    "custom_meta_info = pq_meta.metadata[b\"eos\"]\n",
    "custom_meta_info = json.loads(custom_meta_info)\n",
    "# custom_meta_info\n",
    "\n",
    "for key, val in custom_meta_info.items():\n",
    "    print(f\"{key}: {val}\")\n",
    "\n",
    "df = ddf.compute()\n",
    "\n",
    "multi_tpl = [tuple(col.split(\"_\")) for col in ddf.columns]\n",
    "multi_col = pd.MultiIndex.from_tuples(multi_tpl)\n",
    "i1 = multi_col.get_level_values(0)\n",
    "i1 = [\n",
    "    \"\" if str(idx) in (str(pd.NA), \"nan\", \"\") else idx for idx in i1\n",
    "]  # convert index of level 2 type to int and '' if NA\n",
    "i2 = multi_col.get_level_values(\n",
    "    1\n",
    ")  # must be null string instead of the default pd.NA or np.nan\n",
    "i2 = [\n",
    "    \"\" if str(idx) in (str(pd.NA), \"nan\", \"\") else idx for idx in i2\n",
    "]  # convert index of level 2 type to int and '' if NA\n",
    "i3 = multi_col.get_level_values(\n",
    "    2\n",
    ")  # must be null string instead of the default pd.NA or np.nan\n",
    "i3 = [\n",
    "    \"\" if str(idx) in (str(pd.NA), \"nan\", \"\") else int(idx) for idx in i3\n",
    "]  # convert index of level 2 type to int and '' if NA\n",
    "\n",
    "multi_col = pd.MultiIndex.from_arrays([i1, i2, i3])\n",
    "multi_col.names = [\"tuple\", \"rows\", \"idx\"]\n",
    "df.columns = multi_col\n",
    "\n",
    "# multi_idx = [tuple(idx.split('_')) for idx in ddf.index]\n",
    "# multi_idx = pd.MultiIndex.from_tuples(multi_idx)\n",
    "# i0 = multi_idx.get_level_values(\n",
    "#     0\n",
    "# )  # must be null string instead of the default pd.NA or np.nan\n",
    "# i0 = [\n",
    "#     ''\n",
    "#     if str(idx) in (str(pd.NA), 'nan', '')\n",
    "#     else datetime.strptime(idx, '%Y-%m-%dT%H:%M:%S.%f')\n",
    "#     for idx in i0\n",
    "# ]  # convert index of level 2 type to int and '' if NA\n",
    "# i1 = multi_idx.get_level_values(\n",
    "#     1\n",
    "# )  # must be null string instead of the default pd.NA or np.nan\n",
    "# i1 = [\n",
    "#     ''\n",
    "#     if str(idx) in (str(pd.NA), 'nan', '')\n",
    "#     else datetime.strptime(idx, '%Y-%m-%dT%H:%M:%S.%f')\n",
    "#     for idx in i1\n",
    "# ]  # convert index of level 2 type to int and '' if NA\n",
    "# multi_idx = pd.MultiIndex.from_arrays([i0, i1], names=('episodestart', 'timestamp'))\n",
    "# df.index = multi_idx\n",
    "\n",
    "\n",
    "df = df.set_index([\"vehicle\", \"driver\", \"episodestart\", df.index])\n",
    "df\n",
    "df.columns\n",
    "df.index\n",
    "# len(multi_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Quadratuple from parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_parquet(\n",
    "    \"eos_combine_episodes\",\n",
    "    engine=\"pyarrow\",\n",
    "    compression=\"snappy\",\n",
    "    ignore_metadata_file=False,\n",
    "    infer_divisions=True,\n",
    ")\n",
    "df = ddf.compute()\n",
    "df = df.sort_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_list = []\n",
    "for n in range(ddf.npartitions):\n",
    "    ddf_list.append(ddf.partitions[n])\n",
    "len(ddf_list)\n",
    "type(ddf_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Quadratuple from parquet files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vehicle = \"VB7\"\n",
    "target_driver = \"wang-cheng\"\n",
    "partition = []\n",
    "for i, ddf in enumerate(ddf_list):\n",
    "    # if dfs_episode_new['vehicle']\n",
    "    print(f\"{i}\")\n",
    "    # df = ddf.compute()\n",
    "    # ddf.columns\n",
    "    # ddf.dtypes\n",
    "    vehicle = ddf[\"vehicle__\"].compute()[0]\n",
    "    driver = ddf[\"driver__\"].compute()[0]\n",
    "    # df_episode_new = df_episode_new.append(ddf)\n",
    "    if target_vehicle == vehicle and target_driver == driver:\n",
    "        bHit = True\n",
    "        print(\n",
    "            f'hit {i}{\"st\" if i==1 else \"nd\" if i==2 else \"rd\" if i==3 else \"th\"} partition'\n",
    "        )\n",
    "        # k = i\n",
    "        partition.append(i)\n",
    "\n",
    "        # ddf_list[i] = dd.concat([ddf, df], axis=0)\n",
    "        # append ddf to dfs_episode_new\n",
    "\n",
    "if bHit == False:\n",
    "    print(\"no hit\")\n",
    "    # ddf_list.append(df_episode_new)\n",
    "    # append ddf to dfs_episode_new\n",
    "\n",
    "partition\n",
    "ddf_target = dd.concat([ddf_list[i] for i in partition], axis=0)\n",
    "# ddf_target\n",
    "# # ddf_target.head()\n",
    "df_target = ddf_target.compute()\n",
    "df_target\n",
    "# # df_target\n",
    "\n",
    "# #     c_ep = df_episode_new['ep\n",
    "# #     dfs_episode_new = dfs_episode_newappend(ddf)\n",
    "# # print(f\"splicing dfs_episode_new {c_ep} with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple_sample = ddf_target.sample(frac=0.2).compute()\n",
    "tuple_sample[\"episodestart__\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_target.sample(frac=0.2).compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
